{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import time\n",
    "\n",
    "# Store data as a csv file written out\n",
    "from csv import writer\n",
    "from datetime import datetime\n",
    "\n",
    "# Random integer for more realistic timing for clicks, buttons and searches during scraping\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "# Dataframe stuff\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from seleniumbase import Driver\n",
    "from sqlalchemy import create_engine,exc\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import mysql.connector\n",
    "\n",
    "# Manages Binaries needed for WebDriver without installing anything directly\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "print(\"- Finish importing packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Input your desired search\n",
    "# pos = input(\"What position do you want to search?\").replace(\" \", \"%20\")\n",
    "# loc = input(\"What location do you target?\").replace(\" \", \"%2C%20\")\n",
    "pos = \"Portfolio%20Management\"\n",
    "loc = \"United%20States\"\n",
    "\n",
    "job_description_list = []\n",
    "job_lst = []\n",
    "urls = []\n",
    "skills_lst = []\n",
    "salaries = []\n",
    "working_models = []\n",
    "target_url = \"https://www.linkedin.com/jobs/search/?f_E=2&keywords={}&location={}&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&start={}\"\n",
    "\n",
    "#Target at a specific state in the US\n",
    "# target_url = \"https://www.linkedin.com/jobs/search/?f_E=2&keywords={}&location={}%2C%20United%20States&origin=JOB_SEARCH_PAGE_JOB_FILTER&refresh=true&start={}\" \n",
    "\n",
    "\n",
    "#Initialize the driver\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=C:\\\\Users\\\\Hien Bach\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\n",
    "options.add_argument(\"--profile-directory=Default\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "options.add_argument(\"--remote-debugging-pipe\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "sleep(5)\n",
    "\n",
    "driver.get(target_url.format(pos,loc,0))\n",
    "print(\"- Finish initializing a driver\")\n",
    "sleep(5)\n",
    "\n",
    "#Define number of available jobs\n",
    "src = driver.page_source\n",
    "soup1 = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "num = soup1.find(\"div\", {\"class\": \"jobs-search-results-list__subtitle\"}).find(\"span\").get_text()\n",
    "num_of_pgs = int(num.split(\" \")[0].replace(',','')) // 25\n",
    "sleep(5)\n",
    "\n",
    "#Start scraping\n",
    "#Iterate through each page\n",
    "for i in range(0, num_of_pgs + 1):\n",
    "    \n",
    "    if i <= 39:\n",
    "        driver.get(target_url.format(pos, loc, i * 25))\n",
    "        sleep(5)\n",
    "\n",
    "        #Scroll down for full visible jobs\n",
    "        jobs_block = driver.find_element(By.CSS_SELECTOR, '.jobs-search-results-list')\n",
    "        jobs_list= jobs_block.find_elements(By.CSS_SELECTOR, '.jobs-search-results__list-item')\n",
    "\n",
    "        n = 0\n",
    "        for i in jobs_list:\n",
    "            n+=1\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", jobs_list[n-1])\n",
    "        sleep(5)\n",
    "        \n",
    "        src = driver.page_source\n",
    "        soup2 = BeautifulSoup(src, \"html.parser\")\n",
    "        details = soup2.find_all(\n",
    "            \"li\",\n",
    "            class_=\"ember-view jobs-search-results__list-item occludable-update p0 relative scaffold-layout__list-item\",\n",
    "        )\n",
    "        \n",
    "        #Iterate through each job\n",
    "        for job in details:\n",
    "            \n",
    "            job_title = (\n",
    "                job.find(\"a\", class_=\"disabled ember-view job-card-container__link job-card-list__title job-card-list__title--link\").get_text().strip()\n",
    "            )\n",
    "            company = job.find(\"span\", class_=\"job-card-container__primary-description\").get_text().strip()\n",
    "            location = job.find(\"li\", class_=\"job-card-container__metadata-item\").get_text().strip()\n",
    "            state = location.split(',')[-1][0:3].strip()\n",
    "            work_model = location.split(',')[-1][3:].strip().strip(\"()\")\n",
    "            link_temp = job.find(\"a\", class_=\"disabled ember-view job-card-container__link job-card-list__title job-card-list__title--link\").get(\"href\")\n",
    "            link = \"https://linkedin.com\" + link_temp\n",
    "            jobID = job.get(\"data-occludable-job-id\")\n",
    "            urls.append(link)\n",
    "            job_lst.append([jobID, job_title, company, location, state, work_model, link])\n",
    "\n",
    "        for url in urls:\n",
    "\n",
    "            driver.get(url)\n",
    "            sleep(5)\n",
    "            src = driver.page_source\n",
    "            soup3 = BeautifulSoup(src, \"html.parser\")\n",
    "\n",
    "            try:\n",
    "                salaries.append(soup3.find(\"p\", class_=\"t-16\").get_text().strip())\n",
    "            except Exception:\n",
    "                salaries.append(None)\n",
    "\n",
    "            try:\n",
    "                job_description_list.append(soup3.find(\"div\", id=\"job-details\").get_text().strip())\n",
    "            except Exception:\n",
    "                job_description_list.append(None)\n",
    "\n",
    "            try:\n",
    "                skill_block = driver.find_element(By.CSS_SELECTOR, '.job-details-how-you-match-card__container')\n",
    "                element = skill_block.find_element(By.TAG_NAME, \"button\")\n",
    "                actions = ActionChains(driver)\n",
    "                actions.move_to_element(element).perform()\n",
    "                sleep(5)\n",
    "                \n",
    "                element.click()\n",
    "                sleep(5)\n",
    "                skillList = driver.find_element(By.CLASS_NAME, \"job-details-skill-match-status-list\")\n",
    "                skills_cards = skillList.find_elements(By.TAG_NAME, \"li\")\n",
    "                skills_temp = [s.text.split(\"\\n\")[0].strip() for s in skills_cards]\n",
    "                skills = \", \".join(skills_temp)\n",
    "                skills_lst.append(skills)\n",
    "            except Exception as Error:\n",
    "                print(Error)\n",
    "                skills_lst.append(None)\n",
    "            \n",
    "            # skills_temp = []\n",
    "        \n",
    "        urls = []\n",
    "\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "driver.quit()\n",
    "end = time.time()\n",
    "print(end - start, \"seconds to complete Query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "job_dict= {\n",
    "    \"ID\": [],\n",
    "    \"Title\": [],\n",
    "    \"Company\": [],\n",
    "    \"Location\": [],\n",
    "    \"State\": [],\n",
    "    \"Salary\": [],\n",
    "    \"Type\": [],\n",
    "    \"Skills\": [],\n",
    "    \"Description\": [],\n",
    "    \"Link\": [],\n",
    "    \"Job Board\": [],\n",
    "}\n",
    "for i in range(0, len(job_lst)):\n",
    "    job_dict[\"ID\"].append(job_lst[i][0])\n",
    "    job_dict[\"Title\"].append(job_lst[i][1])\n",
    "    job_dict[\"Company\"].append(job_lst[i][2])\n",
    "    job_dict[\"Location\"].append(job_lst[i][3])\n",
    "    job_dict[\"State\"].append(job_lst[i][4])\n",
    "    job_dict[\"Link\"].append(job_lst[i][6])\n",
    "    job_dict[\"Type\"].append(job_lst[i][5])\n",
    "    job_dict[\"Salary\"].append(salaries[i])\n",
    "    job_dict[\"Skills\"].append(skills_lst[i])\n",
    "    job_dict[\"Description\"].append(job_description_list[i])\n",
    "    job_dict[\"Job Board\"].append(\"Linkedin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(job_dict)\n",
    "\n",
    "#Connect to SQL database\n",
    "engine = create_engine(\"dialect+driver://username:password@host:port/database\", echo=False)\n",
    "connection = engine.connect()\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.iloc[i:i+1].to_sql(\"job_scrape\", con=connection, if_exists=\"append\", index=False)\n",
    "    except exc.IntegrityError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

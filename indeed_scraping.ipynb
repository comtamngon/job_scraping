{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import time\n",
    "\n",
    "# Store data as a csv file written out\n",
    "from csv import writer\n",
    "from datetime import datetime\n",
    "\n",
    "# Random integer for more realistic timing for clicks, buttons and searches during scraping\n",
    "from random import randint\n",
    "from time import sleep\n",
    "\n",
    "import mysql.connector\n",
    "\n",
    "# Dataframe stuff\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from seleniumbase import Driver\n",
    "from sqlalchemy import create_engine,exc\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import mysql.connector\n",
    "\n",
    "\n",
    "# Manages Binaries needed for WebDriver without installing anything directly\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "print(\"- Finish importing packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "#Input your desired search\n",
    "# job_= input(\"What position do you want to search?\").replace(' ','+')\n",
    "# location= input(\"What location do you target?\").replace(' ','+')\n",
    "job_ = \"Financial+Analyst\"\n",
    "location = \"united+states\"\n",
    "\n",
    "job_lst = []\n",
    "job_description_list = []\n",
    "urls = []\n",
    "skills_list = []\n",
    "salaries = []\n",
    "job_models = []\n",
    "target_url = \"https://www.indeed.com/jobs?q={}&l={}&sc=0kf%3Aexplvl%28ENTRY_LEVEL%29%3B&start={}\"\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"user-data-dir=C:\\\\Users\\\\Hien Bach\\\\AppData\\\\Local\\\\Google\\\\Chrome\\\\User Data\")\n",
    "options.add_argument(\"--profile-directory=Profile 3\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")  # Overcome limited resource problems\n",
    "options.add_argument(\"--no-sandbox\")  # Bypass OS security model\n",
    "options.add_argument(\"--remote-debugging-pipe\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "sleep(randint(2, 4))\n",
    "\n",
    "# define number of available jobs\n",
    "driver.get(target_url.format(job_, location, 0))\n",
    "print(\"- Finish initializing a driver\")\n",
    "sleep(randint(2, 4))\n",
    "\n",
    "\n",
    "src = driver.page_source\n",
    "soup = BeautifulSoup(src, \"lxml\")\n",
    "\n",
    "num = soup.find(\"div\", {\"class\": \"jobsearch-JobCountAndSortPane-jobCount\"}).find(\"span\").get_text()\n",
    "num_of_pgs = int(num.split(\" \")[0].replace(',','')) // 15\n",
    "sleep(randint(2, 4))\n",
    "\n",
    "# start scraping\n",
    "for i in range(0, num_of_pgs + 1):\n",
    "    driver.get(target_url.format(job_, location, i * 10))\n",
    "    src = driver.page_source\n",
    "    soup1 = BeautifulSoup(src, \"html.parser\")\n",
    "    jobs = soup1.find_all(\"div\", class_=\"job_seen_beacon\")\n",
    "\n",
    "    for jj in jobs:\n",
    "\n",
    "        job_title = jj.find(\"h2\", class_=\"jobTitle\")\n",
    "        job_key = job_title.find(\"a\").get(\"data-jk\")\n",
    "        lo_co = jj.find(\"div\", class_=\"company_location\")\n",
    "        com = (\n",
    "            lo_co.find(\"div\", {\"data-testid\": \"timing-attribute\"})\n",
    "            .find(\"span\", {\"data-testid\": \"company-name\"})\n",
    "            .get_text()\n",
    "            .strip()\n",
    "        )\n",
    "        lo = (\n",
    "            lo_co.find(\"div\", {\"data-testid\": \"timing-attribute\"})\n",
    "            .find(\"div\", {\"data-testid\": \"text-location\"})\n",
    "            .get_text()\n",
    "            .strip()\n",
    "        )\n",
    "        state = lo.split(\",\")[-1].strip()[0:2]\n",
    "        link_temp = job_title.find(\"a\").get(\"href\")\n",
    "        link = \"https://www.indeed.com/\" + link_temp\n",
    "        job_lst.append([job_title.get_text().strip(), job_key, com, lo, state, link])\n",
    "        urls.append(link)\n",
    "\n",
    "    for url in urls:\n",
    "\n",
    "        driver.get(url)\n",
    "        sleep(randint(2, 4))\n",
    "        src = driver.page_source\n",
    "        soup2 = BeautifulSoup(src, \"html.parser\")\n",
    "\n",
    "        try:\n",
    "            job_description_list.append(soup2.find(\"div\", id=\"jobDescriptionText\").get_text().strip())\n",
    "        except Exception:\n",
    "            job_description_list.append(None)\n",
    "\n",
    "        try:\n",
    "            salaries.append(soup2.find(\"div\", id=\"salaryInfoAndJobType\").find(\"span\", class_=\"css-19j1a75 eu4oa1w0\").get_text().strip())\n",
    "        except Exception:\n",
    "            salaries.append(None)\n",
    "\n",
    "        try:\n",
    "            job_models.append(soup2.find(\"div\", class_=\"css-6z8o9s eu4oa1w0\").get_text().strip())\n",
    "        except Exception:\n",
    "            job_models.append(None)\n",
    "        \n",
    "        try:\n",
    "            skills_block = soup2.find(\"div\", attrs={\"aria-label\":\"Skills\"})\n",
    "            skills_section = skills_block.find(\"ul\", class_=\"js-match-insights-provider-1o7r14h eu4oa1w0\")\n",
    "            try:\n",
    "                show_more = skills_section.find(\"li\", class_=\"js-match-insights-provider-6oegoj eu4oa1w0\")\n",
    "                if show_more.get_text() == '+ show more':\n",
    "\n",
    "                    show_more_button = driver.find_element(By.XPATH, \"//button [contains( text(), '+ show more')]\")\n",
    "\n",
    "                    actions = ActionChains(driver)\n",
    "                    actions.move_to_element(show_more_button).perform()\n",
    "                    sleep(5)\n",
    "                    \n",
    "                    show_more_button.click()\n",
    "                    sleep(5)\n",
    "\n",
    "                    src = driver.page_source\n",
    "                    soup1 = BeautifulSoup(src, \"html.parser\")\n",
    "                    skills_section_temp = soup1.find(\"ul\", class_=\"js-match-insights-provider-1o7r14h eu4oa1w0\")\n",
    "                    skills_cards_temp = skills_section_temp.find_all(\"li\", class_=\"js-match-insights-provider-10zb82q eu4oa1w0\")\n",
    "                    skills_temp = [s.get_text().strip() for s in skills_cards_temp]\n",
    "                    skills = ', '.join(skills_temp)\n",
    "                    skills_list.append(skills)\n",
    "            except Exception:\n",
    "                skills_cards = skills_section.find_all(\"div\", class_=\"js-match-insights-provider-g6kqeb ecydgvn0\")\n",
    "                skills_temp = [s.get_text().strip() for s in skills_cards]\n",
    "                skills = ', '.join(skills_temp)\n",
    "                skills_list.append(skills)\n",
    "        except Exception:       \n",
    "            skills_list.append(None)\n",
    "        skills_temp = []\n",
    "        \n",
    "    urls = []\n",
    "\n",
    "\n",
    "driver.quit()\n",
    "end = time.time()\n",
    "print(end - start, \"seconds to complete Query!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dataframe\n",
    "job_dict= {\n",
    "    \"ID\": [],\n",
    "    \"Title\": [],\n",
    "    \"Company\": [],\n",
    "    \"Location\": [],\n",
    "    \"State\": [],\n",
    "    \"Salary\": [],\n",
    "    \"Type\": [],\n",
    "    \"Skills\": [],\n",
    "    \"Description\": [],\n",
    "    \"Link\": [],\n",
    "    \"Job Board\": [],\n",
    "}\n",
    "for i in range(0, len(job_lst)):\n",
    "    job_dict[\"Title\"].append(job_lst[i][0])\n",
    "    job_dict[\"ID\"].append(job_lst[i][1])\n",
    "    job_dict[\"Company\"].append(job_lst[i][2])\n",
    "    job_dict[\"Location\"].append(job_lst[i][3])\n",
    "    job_dict[\"State\"].append(job_lst[i][4])\n",
    "    job_dict[\"Link\"].append(job_lst[i][5])\n",
    "    job_dict[\"Salary\"].append(salaries[i])\n",
    "    job_dict[\"Type\"].append(job_models[i])\n",
    "    job_dict[\"Skills\"].append(skills_list[i])\n",
    "    job_dict[\"Description\"].append(job_description_list[i])\n",
    "    job_dict[\"Job Board\"].append(\"Indeed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(job_dict)\n",
    "\n",
    "#Connect to SQL database\n",
    "engine = create_engine(\"dialect+driver://username:password@host:port/database\", echo=False)\n",
    "connection = engine.connect()\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.iloc[i:i+1].to_sql(\"job_scrape\", con=connection, if_exists=\"append\", index=False)\n",
    "    except exc.IntegrityError:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
